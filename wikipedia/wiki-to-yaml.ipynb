{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib\n",
      "from urllib2 import urlopen\n",
      "import urllib2\n",
      "import time\n",
      "import wikipedia\n",
      "from bs4 import BeautifulSoup\n",
      "import yaml\n",
      "import json\n",
      "import sys"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 405
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "names = []\n",
      "data = []\n",
      "\n",
      "with open(\"people-list-copy.yaml\", \"r\") as yaml_file:\n",
      "    entries = yaml_file.read().splitlines()\n",
      "    \n",
      "    num = 0\n",
      "    for e in entries:\n",
      "        e.strip(\"- \")\n",
      "        names.append(e) #append names to list\n",
      "#create dictionary to store whether source exists for the person\n",
      "        num += 1\n",
      "#     print num\n",
      "\n",
      "val_keys = sources = [\"Wikipedia\", \"Center for Data Science NYU\"] #add to this as run each scraper\n",
      "source_dict = dict(zip(val_keys, [False]*len(val_keys))) #set all values as false\n",
      "\n",
      "all_d = []\n",
      "i = 0\n",
      "j = 0\n",
      "for n in names[0:3]:\n",
      "    d = {n: source_dict}\n",
      "    all_d.append(d)\n",
      "    #print all_d  # e.g. {Paul Glimcher : {\"Wikipedia\": False, \"Center for Data Science NYU\": False}}\n",
      "    try:\n",
      "        pageurl = urllib.urlopen(\"http://en.wikipedia.org/wiki/\" + n)        \n",
      "        title = wikipedia.page(n)\n",
      "        summary = title.summary\n",
      "        categs = title.categories\n",
      "        outlinks = title.links\n",
      "        sections = title.sections\n",
      "        refs = title.references\n",
      "        # content = title.content\n",
      "        imageurl = title.images\n",
      "        # print p + \" has wiki\"\n",
      "    \n",
      "        #set wikipedia value in dictionary to true\n",
      "        source_dict.update({\"Wikipedia\":True})\n",
      "        i += 1\n",
      "        # data.append({'pageurl': str(pageurl), 'title': title, 'url': pageurl.read(), 'summary': str(summary),\n",
      "        # 'categories': str(categs), 'outlinks': str(outlinks),'sections': str(sections),\n",
      "        # 'references': refs,'content': content, 'imageurl': imageurl})\n",
      "        \n",
      "        data.append({'title': title, 'summary': summary, 'url': 'http://en.wikipedia.org/wiki/' + n,\n",
      "        \t'categories': categs, 'outlinks': outlinks,'sections': sections,\n",
      "        \t'references': refs, 'imageurl': imageurl})  #'content': content,\n",
      "\n",
      "    except wikipedia.exceptions.DisambiguationError as e: #if reach this, they aren't on wikipedia\n",
      "        #print e.options \n",
      "#         print p + \" no wiki\"\n",
      "        j += 0\n",
      "        pass\n",
      "    except wikipedia.exceptions.PageError as pe:\n",
      "#       print p + \" no wiki\"\n",
      "        pass\n",
      "\n",
      "with open('wiki-data.yaml', 'w') as outfile:\n",
      "\toutfile.write( yaml.dump(data, default_flow_style=False) )\n",
      "\tprint \"finished!\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "finished!\n"
       ]
      }
     ],
     "prompt_number": 407
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
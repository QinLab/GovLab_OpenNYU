{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib\n",
      "from urllib2 import urlopen\n",
      "import urllib2\n",
      "import time\n",
      "import wikipedia\n",
      "from bs4 import BeautifulSoup\n",
      "import yaml\n",
      "import json\n",
      "import sys"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 305
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "people = []\n",
      "\n",
      "with open(\"people-list-copy.yaml\", \"r\") as yaml_file:\n",
      "    entries = yaml_file.read().splitlines()\n",
      "    \n",
      "    num = 0\n",
      "    for e in entries:\n",
      "        people.append(e) #append names to list\n",
      "        num += 1\n",
      "    print num\n",
      "for p in people:\n",
      "    p.strip(\"- \")\n",
      "\n",
      "#create dictionary to store whether source exists for the person\n",
      "val_keys = sources = [\"Wikipedia\", \"Center for Data Science NYU\"] #add to this as run each scraper\n",
      "source_dict = dict(zip(val_keys, [False]*len(val_keys))) #set all values as false\n",
      "\n",
      "all_d = []\n",
      "i = 0\n",
      "j = 0\n",
      "for p in people[0:1]:\n",
      "    d = {p: source_dict}\n",
      "    all_d.append(d)\n",
      "    #print all_d  # e.g. {Paul Glimcher : {\"Wikipedia\": False, \"Center for Data Science NYU\": False}}\n",
      "    try:\n",
      "        pageurl = urllib.urlopen(\"http://en.wikipedia.org/wiki/\" + p)        \n",
      "        title = wikipedia.page(p)\n",
      "        url = title.url\n",
      "        summary = title.summary\n",
      "        categs = title.categories\n",
      "        outlinks = title.links\n",
      "        sections = title.sections\n",
      "        refs = title.references\n",
      "        content = title.content    # convert to strings ???\n",
      "        image = title.images\n",
      "#         if len(outlinks) > 0:\n",
      "#             print outlinks\n",
      "        # print p + \" has wiki\"\n",
      "    \n",
      "        #set wikipedia value in dictionary to true\n",
      "        source_dict.update({\"Wikipedia\":True})\n",
      "        i += 1\n",
      "        \n",
      "    except wikipedia.exceptions.DisambiguationError as e: #if reach this, they aren't on wikipedia\n",
      "        #print e.options \n",
      "#         print p + \" no wiki\"\n",
      "        j += 0\n",
      "        pass\n",
      "    except wikipedia.exceptions.PageError as pe:\n",
      "#       print p + \" no wiki\"\n",
      "        pass\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "84\n"
       ]
      }
     ],
     "prompt_number": 350
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 316
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#         page = \"http://en.wikipedia.org/wiki/\" + p\n",
      "#         soup = BeautifulSoup(urllib.urlopen(page))\n",
      "#         print soup.find_all(\"h3\")\n",
      "\n",
      "# try:\n",
      "#     query = \"Paul Glimcher\"\n",
      "#     url = urllib.urlopen(\"http://en.wikipedia.org/wiki/\" + query) #fetch page for topic\n",
      "#     title = wikipedia.page(query)\n",
      "#     print title\n",
      "#     summary = title.summary\n",
      "#     categ = title.categories\n",
      "#     links = title.links\n",
      "#     sections = title.sections\n",
      "#     print \"sections: \" + sections\n",
      "#     suggestions = title.suggest\n",
      "#     print suggestions\n",
      "#     content = title.content\n",
      "#     print content\n",
      "    \n",
      "# except:\n",
      "# #     print \"Page not found for \" + query + \". Try another query.\"\n",
      "#      pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}